{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                CommentId      VideoId                                                                                              Text  IsToxic  IsAbusive  IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism                                                                                   cleaned_text                                                             semantic_tokens                                        processed_text\n",
      "592  UghS3N0TuPkVzXgCoAEC  dG7mZQvaQDk  Too much logic and intelligence.  She must be mixed race with some intelligent white DNA in her.     True      False     False          False      False          True      True          False     False         False            False         False  TOO MUCH LOGIC AND INTELLIGENCE SHE MUST BE MIXED RACE WITH SOME INTELLIGENT WHITE DNA IN HER  ['MUCH', 'LOGIC', 'INTELLIGENCE', 'MIXED', 'RACE', 'INTELLIGENT', 'WHITE']  MUCH LOGIC INTELLIGENCE MIXED RACE INTELLIGENT WHITE\n",
      "Index(['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat',\n",
      "       'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist',\n",
      "       'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate',\n",
      "       'IsRadicalism', 'cleaned_text', 'semantic_tokens', 'processed_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/processed/processed_train_data.csv')\n",
    "val_df = pd.read_csv('../data/processed/processed_val_data.csv')\n",
    "test_df = pd.read_csv('../data/processed/processed_test_data.csv')\n",
    "filtro=train_df[train_df['IsHatespeech']==True]\n",
    "print(filtro.tail(1))\n",
    "print(train_df.columns)\n",
    "# Lista de columnas a conservar\n",
    "columns_to_keep = ['semantic_tokens', 'IsHatespeech', 'IsToxic', 'IsAbusive', 'IsThreat',\n",
    "                   'IsProvocative', 'IsObscene', 'IsRacist', 'IsNationalist', 'IsSexist',\n",
    "                   'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n",
    "\n",
    "# Filtrar los DataFrames para conservar solo las columnas seleccionadas\n",
    "train_df_filtered = train_df[columns_to_keep]\n",
    "val_df_filtered = val_df[columns_to_keep]\n",
    "test_df_filtered = test_df[columns_to_keep]\n",
    "\n",
    "# Guardar los DataFrames resultantes \n",
    "train_df_filtered.to_csv('../data/processed/filtered_train_data.csv', index=False) \n",
    "val_df_filtered.to_csv('../data/processed/filtered_val_data.csv', index=False) \n",
    "test_df_filtered.to_csv('../data/processed/filtered_test_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 50 palabras más comunes en el dataset de entrenamiento son: [(\"'BLACK',\", 162), (\"'PEOPLE',\", 128), (\"'WHITE',\", 96), (\"'POLICE',\", 89), (\"'COP',\", 78), (\"'OFFICER',\", 77), (\"'GET',\", 76), (\"'SAY',\", 69), (\"'SHOOT',\", 69), (\"'GO',\", 69), (\"'MAKE',\", 55), (\"'VIDEO',\", 50), (\"'KNOW',\", 44), (\"'MAN',\", 38), (\"'FUCK',\", 38), (\"'KILL',\", 38), (\"'RACIST',\", 38), (\"'SEE',\", 36), (\"'LIFE',\", 35), (\"'NEED',\", 33), (\"'TRY',\", 32), (\"'TAKE',\", 32), (\"'FACT',\", 32), (\"'TIME',\", 31), (\"'RACE',\", 30), (\"'THINK',\", 29), (\"'LOOK',\", 29), (\"'COME',\", 29), (\"'USE',\", 29), (\"'BACK',\", 28), (\"'MATTER',\", 28), (\"'PROTEST',\", 27), (\"'STILL',\", 27), (\"'WANT',\", 27), (\"'RIGHT',\", 27), (\"'RIOT',\", 26), (\"'STORE',\", 25), (\"'LET',\", 24), (\"'GUN',\", 24), (\"'GUY',\", 24), (\"'EVEN',\", 24), (\"'SHIT',\", 24), (\"'HAND',\", 23), (\"'CASE',\", 22), (\"'THUG',\", 22), (\"'STUPID',\", 21), (\"'TELL',\", 21), (\"'SHOW',\", 21), (\"'YEAR',\", 21), (\"'CALL',\", 21)]\n",
      "Las 50 palabras más comunes en el dataset de validación son: [(\"'BLACK',\", 63), (\"'POLICE',\", 36), (\"'GET',\", 35), (\"'PEOPLE',\", 34), (\"'COP',\", 30), (\"'GO',\", 29), (\"'WHITE',\", 28), (\"'OFFICER',\", 28), (\"'MAKE',\", 22), (\"'SAY',\", 22), (\"'SHOOT',\", 22), (\"'TIME',\", 21), (\"'NEED',\", 17), (\"'TAKE',\", 16), (\"'GUY',\", 16), (\"'LIFE',\", 15), (\"'KILL',\", 15), (\"'GOOD',\", 15), (\"'VIDEO',\", 14), (\"'SEE',\", 14), (\"'THINK',\", 14), (\"'KNOW',\", 13), (\"'USE',\", 13), (\"'GUN',\", 13), (\"'THING',\", 13), (\"'GIVE',\", 13), (\"'TRY',\", 12), (\"'BROWN',\", 12), (\"'COMMUNITY',\", 12), (\"'FUCK',\", 12), (\"'COME',\", 12), (\"'MAN',\", 11), (\"'WORLD',\", 11), (\"'WAY',\", 11), (\"'WANT',\", 11), (\"'YEAR',\", 11), (\"'FERGUSON',\", 11), (\"'PUT',\", 10), (\"'CALL',\", 10), (\"'RACIST',\", 10), (\"'FACT',\", 10), (\"'HAPPEN',\", 10), (\"'REAL',\", 10), (\"'RACE',\", 10), (\"'PROTEST',\", 9), (\"'LIVE',\", 9), (\"'HAND',\", 9), (\"'HIGH',\", 9), (\"'RAP',\", 9), (\"'BURN',\", 9)]\n",
      "Las 50 palabras más comunes en el dataset de prueba son: [(\"'BLACK',\", 58), (\"'PEOPLE',\", 53), (\"'COP',\", 31), (\"'POLICE',\", 28), (\"'GET',\", 26), (\"'SHOOT',\", 24), (\"'GO',\", 23), (\"'WHITE',\", 20), (\"'KILL',\", 18), (\"'GUY',\", 17), (\"'SAY',\", 17), (\"'KNOW',\", 16), (\"'MAKE',\", 15), (\"'SEE',\", 14), (\"'COME',\", 14), (\"'WANT',\", 14), (\"'MAN',\", 13), (\"'RUN',\", 12), (\"'EVEN',\", 12), (\"'VIDEO',\", 11), (\"'TIME',\", 11), (\"'ASS',\", 11), (\"'CRIMINAL',\", 11), (\"'OFFICER',\", 11), (\"'KID',\", 11), (\"'RACE',\", 10), (\"'RIOT',\", 9), (\"'RIGHT',\", 9), (\"'PERSON',\", 9), (\"'THING',\", 9), (\"'BAD',\", 9), (\"'FUCK',\", 9), (\"'STOP',\", 9), (\"'RACIST',\", 9), (\"'THINK',\", 9), (\"'START',\", 8), (\"'GUN',\", 8), (\"'LIFE',\", 8), (\"'CAUSE',\", 8), (\"'FERGUSON',\", 8), (\"'USE',\", 8), (\"'HUMAN',\", 8), (\"'LET',\", 8), (\"'BITCH',\", 8), (\"'REALLY',\", 8), (\"'HAPPEN',\", 8), (\"'LISTEN',\", 7), (\"'LITTLE',\", 7), (\"'GIRL',\", 7), (\"'CARE',\", 7)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Función para encontrar las 50 palabras más comunes en una columna de texto\n",
    "def find_most_common_words(df, column_name, num_words=50):\n",
    "    # Concatenar todos los tokens en una sola lista\n",
    "    all_tokens = [token for tokens in df[column_name] for token in tokens.split()]\n",
    "    \n",
    "    # Contar la frecuencia de cada token\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Encontrar las palabras más comunes\n",
    "    most_common_words = token_counts.most_common(num_words)\n",
    "    return most_common_words\n",
    "\n",
    "\n",
    "\n",
    "# Encontrar las 50 palabras más comunes en cada dataset\n",
    "train_most_common_words = find_most_common_words(train_df, 'semantic_tokens')\n",
    "val_most_common_words = find_most_common_words(val_df, 'semantic_tokens')\n",
    "test_most_common_words = find_most_common_words(test_df, 'semantic_tokens')\n",
    "\n",
    "print(\"Las 50 palabras más comunes en el dataset de entrenamiento son:\", train_most_common_words)\n",
    "print(\"Las 50 palabras más comunes en el dataset de validación son:\", val_most_common_words)\n",
    "print(\"Las 50 palabras más comunes en el dataset de prueba son:\", test_most_common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
